{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NFrTVLpfOu4lld4axF61JfdG1dt1b6Rm",
      "authorship_tag": "ABX9TyNJ1hQkTynepMD4N2W500I6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jigyass/Data-Privacy-and-Data-Security-Models/blob/main/Exponential_Mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQgpvxBjZE_v",
        "outputId": "6c3cc4b2-e989-4126-c739-34b86a37d7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Data_Privacy_and_Data_Security/adult.zip\n",
            "replace Index? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: Index                   \n",
            "  inflating: adult.data              \n",
            "  inflating: adult.names             \n",
            "  inflating: adult.test              \n",
            "  inflating: old.adult.names         \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/Data_Privacy_and_Data_Security/adult.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy.stats import entropy\n",
        "from collections import Counter\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "5bY2a9tMZRKd"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('adult.data')"
      ],
      "metadata": {
        "id": "KEX4ydSzZSqO"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['Age', 'Workclass', 'Fnlwgt', 'Education', 'Education-Num', 'Marital-Status', 'Occupation', 'Relationship', 'Race', 'Sex', 'Capital-Gain', 'Capital-Loss', 'Hours-Per-Week', 'Native-Country', 'Income']"
      ],
      "metadata": {
        "id": "2YLkrJWGZUar"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Original Dataset\n",
        "original_most_frequent = df['Education'].value_counts().idxmax()\n",
        "print(f\"Most frequent 'Education' in original dataset: {original_most_frequent}\")\n",
        "\n",
        "# 2. Removing a record with the most frequent \"Education\"\n",
        "df1 = df[df['Education'] != original_most_frequent].copy()\n",
        "df1_most_frequent = df1['Education'].value_counts().idxmax()\n",
        "print(f\"Most frequent 'Education' in df1: {df1_most_frequent}\")\n",
        "\n",
        "# 3. Removing a record with the second most frequent \"Education\"\n",
        "second_most_frequent = df['Education'].value_counts().index[1]\n",
        "df2 = df[df['Education'] != second_most_frequent].copy()\n",
        "df2_most_frequent = df2['Education'].value_counts().idxmax()\n",
        "print(f\"Most frequent 'Education' in df2: {df2_most_frequent}\")\n",
        "\n",
        "# 4. Removing a record with the least frequent \"Education\"\n",
        "least_frequent = df['Education'].value_counts().idxmin()\n",
        "df3 = df[df['Education'] != least_frequent].copy()\n",
        "df3_most_frequent = df3['Education'].value_counts().idxmax()\n",
        "print(f\"Most frequent 'Education' in df3: {df3_most_frequent}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoZILDqNwdr_",
        "outputId": "43343d6b-abb3-463a-e9d2-f9a24231c382"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent 'Education' in original dataset:  HS-grad\n",
            "Most frequent 'Education' in df1:  Some-college\n",
            "Most frequent 'Education' in df2:  HS-grad\n",
            "Most frequent 'Education' in df3:  HS-grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding the 'Education' column\n",
        "df['Education'] = df['Education'].astype('category')\n",
        "df['Education_Cat'] = df['Education'].cat.codes\n",
        "\n",
        "# Doing the same for the modified DataFrames (df1, df2, df3)\n",
        "df1['Education'] = df1['Education'].astype('category')\n",
        "df1['Education_Cat'] = df1['Education'].cat.codes\n",
        "\n",
        "df2['Education'] = df2['Education'].astype('category')\n",
        "df2['Education_Cat'] = df2['Education'].cat.codes\n",
        "\n",
        "df3['Education'] = df3['Education'].astype('category')\n",
        "df3['Education_Cat'] = df3['Education'].cat.codes\n"
      ],
      "metadata": {
        "id": "2yq9i3Aqwjxy"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_mechanism(df, epsilon):\n",
        "    counts = df['Education'].value_counts()\n",
        "    keys = counts.keys().tolist()\n",
        "    utilities = counts.values\n",
        "    sensitivity = 1.0 / len(keys)  # 1/n sensitivity\n",
        "\n",
        "    # Handle large numbers to avoid overflow\n",
        "    max_utility = np.max(utilities)\n",
        "    scaled_utilities = utilities - max_utility\n",
        "\n",
        "    # Calculate probabilities\n",
        "    try:\n",
        "        probabilities = np.exp((epsilon * scaled_utilities) / (2 * sensitivity))\n",
        "    except OverflowError:\n",
        "        probabilities = np.zeros_like(utilities)\n",
        "\n",
        "    # Handle NaN and Inf\n",
        "    probabilities[np.isnan(probabilities)] = 1e-10\n",
        "    probabilities[np.isinf(probabilities)] = 1e+10\n",
        "\n",
        "    # Normalize probabilities\n",
        "    sum_probabilities = np.sum(probabilities)\n",
        "    if sum_probabilities == 0:\n",
        "        sum_probabilities = 1e-10\n",
        "    probabilities /= sum_probabilities\n",
        "\n",
        "    return np.random.choice(keys, p=probabilities)\n",
        "\n",
        "# For Îµ = 0.5\n",
        "epsilon = 0.5\n",
        "\n",
        "# Generate 1,000 random results for each dataset\n",
        "n = 1000\n",
        "original_results = [exponential_mechanism(df, epsilon) for _ in range(n)]\n",
        "df1_results = [exponential_mechanism(df1, epsilon) for _ in range(n)]\n",
        "df2_results = [exponential_mechanism(df2, epsilon) for _ in range(n)]\n",
        "df3_results = [exponential_mechanism(df3, epsilon) for _ in range(n)]"
      ],
      "metadata": {
        "id": "3e9d8FalxLKb"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.5  # epsilon value for differential privacy\n",
        "n = 1000  # Number of random results to generate\n",
        "\n",
        "# Generate 1,000 random results for the original dataset\n",
        "original_results = [exponential_mechanism(df, epsilon) for _ in range(n)]\n",
        "\n",
        "# Remove a record with the most frequent 'Education'\n",
        "most_frequent_education = df['Education'].value_counts().idxmax()\n",
        "df1 = df[df['Education'] != most_frequent_education].copy()\n",
        "df1_results = [exponential_mechanism(df1, epsilon) for _ in range(n)]\n",
        "\n",
        "# Remove a record with the second most frequent 'Education'\n",
        "second_most_frequent_education = df['Education'].value_counts().index[1]\n",
        "df2 = df[df['Education'] != second_most_frequent_education].copy()\n",
        "df2_results = [exponential_mechanism(df2, epsilon) for _ in range(n)]\n",
        "\n",
        "# Remove a record with the least frequent 'Education'\n",
        "least_frequent_education = df['Education'].value_counts().idxmin()\n",
        "df3 = df[df['Education'] != least_frequent_education].copy()\n",
        "df3_results = [exponential_mechanism(df3, epsilon) for _ in range(n)]\n"
      ],
      "metadata": {
        "id": "lSSvZi2hypvj"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_epsilon_indistinguishability(results1, results2, epsilon):\n",
        "    # Count frequency of each unique value in both result sets\n",
        "    unique1, counts1 = np.unique(results1, return_counts=True)\n",
        "    unique2, counts2 = np.unique(results2, return_counts=True)\n",
        "\n",
        "    # Create a dictionary for easier lookup\n",
        "    freq_dict1 = dict(zip(unique1, counts1))\n",
        "    freq_dict2 = dict(zip(unique2, counts2))\n",
        "\n",
        "    # Calculate probabilities\n",
        "    total1 = np.sum(counts1)\n",
        "    total2 = np.sum(counts2)\n",
        "\n",
        "    for key in set(unique1).union(unique2):\n",
        "        prob1 = freq_dict1.get(key, 0) / total1\n",
        "        prob2 = freq_dict2.get(key, 0) / total2\n",
        "\n",
        "        # Check if the ratios are bounded by e^epsilon and e^-epsilon\n",
        "        if prob1 == 0 or prob2 == 0:\n",
        "            continue  # Ignore zero probabilities\n",
        "\n",
        "        ratio = prob1 / prob2\n",
        "        if ratio > np.exp(epsilon) or ratio < np.exp(-epsilon):\n",
        "            print(f\"Failed indistinguishability check for value {key}\")\n",
        "            return False\n",
        "\n",
        "    print(\"Passed indistinguishability check\")\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "cfE-_rfOyzr9"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.5  # Set epsilon to 0.5 for part (b)\n",
        "\n",
        "check_epsilon_indistinguishability(original_results, df1_results, epsilon)\n",
        "check_epsilon_indistinguishability(original_results, df2_results, epsilon)\n",
        "check_epsilon_indistinguishability(original_results, df3_results, epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qClot1Vb0suW",
        "outputId": "7c401521-4436-4fc3-8554-66834cf5d344"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed indistinguishability check\n",
            "Passed indistinguishability check\n",
            "Passed indistinguishability check\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1.0  # New epsilon value\n",
        "n = 1000  # Number of samples\n",
        "\n",
        "original_results_1 = [exponential_mechanism(df, epsilon) for _ in range(n)]\n",
        "df1_results_1 = [exponential_mechanism(df1, epsilon) for _ in range(n)]\n",
        "df2_results_1 = [exponential_mechanism(df2, epsilon) for _ in range(n)]\n",
        "df3_results_1 = [exponential_mechanism(df3, epsilon) for _ in range(n)]\n"
      ],
      "metadata": {
        "id": "IR_4AQYMzx3s"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1.0\n",
        "\n",
        "check_epsilon_indistinguishability(original_results_1, df1_results_1, epsilon)\n",
        "check_epsilon_indistinguishability(original_results_1, df2_results_1, epsilon)\n",
        "check_epsilon_indistinguishability(original_results_1, df3_results_1, epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrwI2gmz0d_F",
        "outputId": "e138acf8-413f-4b56-b828-fe03fbd44047"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed indistinguishability check\n",
            "Passed indistinguishability check\n",
            "Passed indistinguishability check\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Jensen-Shannon divergence\n",
        "def jensen_shannon_divergence(p, q):\n",
        "    p = np.array(p)\n",
        "    q = np.array(q)\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "# Calculate empirical probabilities\n",
        "def empirical_probabilities(results, num_unique_values):\n",
        "    counts = np.bincount(results)\n",
        "    return counts / len(results)"
      ],
      "metadata": {
        "id": "5KrQ85W41chx"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_results = pd.DataFrame({\n",
        "    'Original_Îµ=0.5': original_results,\n",
        "    'Modified_most_Îµ=0.5': df1_results,\n",
        "    'Modified_second_Îµ=0.5': df2_results,\n",
        "    'Modified_least_Îµ=0.5': df3_results,\n",
        "    'Original_Îµ=1': original_results_1,\n",
        "    'Modified_most_Îµ=1': df1_results_1,\n",
        "    'Modified_second_Îµ=1': df2_results_1,\n",
        "    'Modified_least_Îµ=1': df3_results_1\n",
        "})\n",
        "# Export the combined_results DataFrame to a CSV file\n",
        "combined_results.to_csv('combined_results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Ze8QqsYK3VLw"
      },
      "execution_count": 129,
      "outputs": []
    }
  ]
}